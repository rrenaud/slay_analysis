{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/alexdriedger/SlayTheSpireFightPredictor/blob/master/STSFightPredictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import numpy as np\n",
    "import collections\n",
    "import datetime\n",
    "import os\n",
    "import pprint\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate, Average, Embedding, Lambda\n",
    "#from tensorflow.keras.layers.merge import concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CARD_INFO = json.load(open('cards_2020_11_14.json', 'r'))\n",
    "RELIC_INFO = json.load(open('relics_2020_11_14.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a0SbaW697MDP",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ascension': 20,\n",
      " 'cards': ['Strike_B',\n",
      "           'Strike_B',\n",
      "           'Strike_B',\n",
      "           'Strike_B',\n",
      "           'Defend_B',\n",
      "           'Defend_B',\n",
      "           'Defend_B',\n",
      "           'Defend_B',\n",
      "           'Zap',\n",
      "           'Dualcast',\n",
      "           'AscendersBane',\n",
      "           'HandOfGreed',\n",
      "           'Turbo'],\n",
      " 'character': 'DEFECT',\n",
      " 'damage_taken': 0,\n",
      " 'enemies': 'Cultist',\n",
      " 'entering_hp': 64,\n",
      " 'floor': 3,\n",
      " 'max_hp': 71,\n",
      " 'not_picked': ['Melter', 'FTL'],\n",
      " 'picked': 'Leap',\n",
      " 'potion_used': False,\n",
      " 'relics': ['Cracked Core', 'FaceOfCleric']}\n",
      "Data has 141925 samples.\n"
     ]
    }
   ],
   "source": [
    "cached_data = False\n",
    "\n",
    "if cached_data:\n",
    "    loaded_data = np.load('cached_comp_data.npz')\n",
    "    X = loaded_data['X']\n",
    "    Y = loaded_data['Y']\n",
    "\n",
    "else:\n",
    "    json_data = list()\n",
    "    for root, dirs, files in os.walk('a20_act1_defect'):\n",
    "        for fname in files:\n",
    "            path = os.path.join(root, fname)\n",
    "            json_data.extend(json.load(open(path)))\n",
    "    pprint.pprint(json_data[0])\n",
    "\n",
    "\n",
    "if cached_data is False:\n",
    "  print(f'Data has {len(json_data)} samples.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_6nEFapfCLp"
   },
   "source": [
    "# Pre-Process Data\n",
    "\n",
    "Load the json training examples and process them into vectors that can be used in a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.3862944, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "cce = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=False,\n",
    "    label_smoothing=0,\n",
    "    reduction=\"auto\",\n",
    "    name=\"categorical_crossentropy\",\n",
    ")\n",
    "\n",
    "y_true = [[0, 1, 0], [0, 0, 1]]\n",
    "y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
    "cce(y_true, y_pred).numpy()\n",
    "#1.177\n",
    "\n",
    "# only_1s([[1, -1, 0], [0, 1, -1]])\n",
    "# array([[1, 0, 0],\n",
    "#       [0, 1, 0]], dtype=int32)>\n",
    "def only_1s(x):\n",
    "    return tf.where(tf.equal(x, 1), tf.ones_like(x), tf.zeros_like(x))\n",
    "\n",
    "#dists = tf.constant([[.3,   .3,    .4],   [.2,    .5,    .3]])\n",
    "#masks = tf.constant([[True, True, False], [False, True, True]])\n",
    "#mask_renormalize_dist(dists, masks)\n",
    "#array([[0.5  , 0.5  , 0.   ],\n",
    "#       [0.   , 0.625, 0.375]], dtype=float32)>\n",
    "def mask_renormalize_dist(dist, mask):\n",
    "    masked_dist = dist * tf.cast(mask, tf.float32)\n",
    "    norms = tf.norm(masked_dist, ord=1, axis=1)\n",
    "    inv_norms = tf.transpose([tf.divide(tf.ones_like(norms), norms)])\n",
    "    return tf.multiply(masked_dist, inv_norms)\n",
    "\n",
    "def masked_cce(y_true, y_pred):\n",
    "    # find the valid locations, (y_pred == 1 or -1)\n",
    "    # turn y_pred -> 0 at invalid locations\n",
    "    # map -1 in y_true to 0.\n",
    "    # compute cce \n",
    "    allowed_choices = tf.not_equal(y_true, 0)\n",
    "    y_pred_renormalized = mask_renormalize_dist(y_pred, allowed_choices)\n",
    "    \n",
    "    y_true_unrestricted = only_1s(y_true)\n",
    "    return cce(y_true_unrestricted, y_pred_renormalized)\n",
    "\n",
    "\n",
    "assert(masked_cce([[-1, 1, -1, 0], [1, -1, -1, 0]], [[.1, .2, .1, .5], [.3, .1, 0, .6]]).numpy()\n",
    "       == cce([    [0, 1, 0],      [1, 0, 0]],      [[.2, .4, .2],     [.75, .25, 0]]).numpy())\n",
    "\n",
    "print(cce([[1,0,0,0]], [[.25, .25, .25, .25]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upgrade_card(c):\n",
    "    return c + \"+1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "in31BTFUe0MF"
   },
   "outputs": [],
   "source": [
    "# Categories for one hot encoder. Categories are in alphabetical order and is the order used by OneHotEncoder\n",
    "ALL_CARDS = [c for c in CARD_INFO if CARD_INFO[c]['color'] in ['BLUE', 'COLORLESS', 'CURSE']] + ['Strike', 'Defend']\n",
    "ALL_CARDS = ALL_CARDS + [upgrade_card(c) for c in ALL_CARDS]\n",
    "ALL_RELICS = list(RELIC_INFO.keys())\n",
    "ALL_ENCOUNTERS = ['2 Fungi Beasts', '2 Louse', '2 Orb Walkers', '2 Thieves', '3 Byrds', '3 Cultists', '3 Darklings', '3 Louse', '3 Sentries', '3 Shapes', '4 Byrds', '4 Shapes', 'Apologetic Slime', 'Automaton', 'Awakened One', 'Blue Slaver', 'Book of Stabbing', 'Centurion and Healer', 'Champ', 'Chosen', 'Chosen and Byrds', 'Collector', 'Colosseum Nobs', 'Colosseum Slavers', 'Cultist', 'Cultist and Chosen', 'Donu and Deca', 'Exordium Thugs', 'Exordium Wildlife', 'Flame Bruiser 1 Orb', 'Flame Bruiser 2 Orb', 'Giant Head', 'Gremlin Gang', 'Gremlin Leader', 'Gremlin Nob', 'Hexaghost', 'Jaw Worm', 'Jaw Worm Horde', 'Lagavulin', 'Lagavulin Event', 'Large Slime', 'Looter', 'Lots of Slimes', 'Masked Bandits', 'Maw', 'Mind Bloom Boss Battle', 'Mysterious Sphere', 'Nemesis', 'Orb Walker', 'Red Slaver', 'Reptomancer', 'Sentry and Sphere', 'Shell Parasite', 'Shelled Parasite and Fungi', 'Shield and Spear', 'Slaver and Parasite', 'Slavers', 'Slime Boss', 'Small Slimes', 'Snake Plant', 'Snecko', 'Snecko and Mystics', 'Sphere and 2 Shapes', 'Spheric Guardian', 'Spire Growth', 'The Eyes', 'The Guardian', 'The Heart', 'The Mushroom Lair', 'Time Eater', 'Transient', 'Writhing Mass']\n",
    "ALL_CHARACTERS = ['DEFECT', 'IRONCLAD', 'THE_SILENT', 'WATCHER']\n",
    "ALL_CARD_CHOICES = ALL_CARDS + ['Singing Bowl', 'SKIP']\n",
    "\n",
    "\n",
    "def make_key(l):\n",
    "    indexes = {}\n",
    "    for index, item in enumerate(l):\n",
    "        assert item not in indexes, (item, l)\n",
    "        indexes[item] = index\n",
    "    return indexes\n",
    "    \n",
    "ALL_CARDS_KEY = make_key(ALL_CARDS)\n",
    "ALL_RELICS_KEY = make_key(ALL_RELICS)\n",
    "ALL_CHARACTERS_KEY = make_key(ALL_CHARACTERS)\n",
    "ALL_ENCOUNTERS_KEY = make_key(ALL_ENCOUNTERS)\n",
    "ALL_CARD_CHOICES_KEY = make_key(ALL_CARD_CHOICES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wF0lHAf6yT2U"
   },
   "source": [
    "## Pre-Process with Loops\n",
    "\n",
    "Iterate through each training example and turn each field in the json data into each part of the vector needed for training.\n",
    "\n",
    "This will later be optomized into vectorized code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rWNjgCWfMcsb"
   },
   "outputs": [],
   "source": [
    "def encode_list2(list_to_encode, encoder_key):\n",
    "    ret = np.zeros(len(encoder_key))\n",
    "    for item in list_to_encode:\n",
    "        ret[encoder_key[item]] += 1\n",
    "    return ret\n",
    "\n",
    "def encode_single2(value, encoder_key):\n",
    "    return encode_list2([value], encoder_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qZfcBL7gwCgV"
   },
   "outputs": [],
   "source": [
    "def generalize_strikes_and_defends(cards):\n",
    "  \"\"\"\n",
    "  Modifies any character specific Strikes and Defends (eg. Strike_R) into\n",
    "  general Strikes and Defends(Strike)\n",
    "  \"\"\"\n",
    "  for i, s in enumerate(cards):\n",
    "    if s.startswith('Strike_') or s.startswith('Defend_'):\n",
    "      cards[i] = re.sub('_.', '', s)\n",
    "  return cards\n",
    "\n",
    "def encode_cards(cards):\n",
    "  return encode_list2(generalize_strikes_and_defends(cards), ALL_CARDS_KEY)\n",
    "\n",
    "def encode_relics(relics):\n",
    "  return encode_list2(relics, ALL_RELICS_KEY)\n",
    "\n",
    "def encode_encounter(encounter):\n",
    "  return encode_single2(encounter, ALL_ENCOUNTERS_KEY)\n",
    "\n",
    "def encode_character(character):\n",
    "  return encode_single2(character, ALL_CHARACTERS_KEY)\n",
    "\n",
    "def encode_card_choice(sample):\n",
    "    not_picked = list(sample['not_picked'])\n",
    "    if sample['picked'] != 'SKIP':\n",
    "        not_picked.append('SKIP')\n",
    "    return (encode_single2(sample['picked'], ALL_CARD_CHOICES_KEY) \n",
    "            - encode_list2(not_picked, ALL_CARD_CHOICES_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(json_data[0]['picked'])\n",
    "# print(json_data[0]['not_picked'])\n",
    "# x = encode_card_choice(json_data[0])\n",
    "# print(np.nonzero(x))\n",
    "# print(x[np.nonzero(x)])\n",
    "# print(ALL_CARD_CHOICES_KEY['Leap'], ALL_CARD_CHOICES_KEY['Melter'], \n",
    "#       ALL_CARD_CHOICES_KEY['FTL'], ALL_CARD_CHOICES_KEY['SKIP'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AZqDcufs3I9M"
   },
   "outputs": [],
   "source": [
    "\n",
    "def encode_sample_with_loop(sample):\n",
    "  \"\"\"\n",
    "  Encode a single sample into a 1D vector\n",
    "  \"\"\"\n",
    "  cards = encode_cards(sample['cards'])\n",
    "  relics = encode_relics(sample['relics'])\n",
    "  #relics = encode_relics(['Burning Blood'])\n",
    "  encounter = encode_encounter(sample['enemies'])\n",
    "  num_and_bool_data = np.array([sample['max_hp'] / 100.0, sample['entering_hp'] / 100.0, sample['ascension'], int(sample['potion_used'] == 'true')])  \n",
    "  return np.concatenate((cards, relics, encounter, num_and_bool_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_encoded_sample(row):\n",
    "    cards = []\n",
    "    relics = []\n",
    "    lac =  len(ALL_CARDS)\n",
    "    lar = len(ALL_RELICS)\n",
    "    for index, quantity in enumerate(row):\n",
    "        if quantity == 0:\n",
    "            continue\n",
    "        if index < lac:\n",
    "            cards.extend([ALL_CARDS[index]] * int(quantity))\n",
    "        elif index < lac + lar:\n",
    "            relics.extend([ALL_RELICS[index - lac]] * int(quantity))\n",
    "    return cards, relics\n",
    "\n",
    "# a = json_data[0]\n",
    "# enc = encode_sample_with_loop(a)\n",
    "# b = decode_encoded_sample(enc)\n",
    "# pprint.pprint(a)\n",
    "# pprint.pprint(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1SaM_GSp3N-V"
   },
   "outputs": [],
   "source": [
    "# Set to True to test embedding experiments\n",
    "USE_EMBEDDING = False\n",
    "\n",
    "def preprocess_with_loop(data, log=True, log_freq=1000):\n",
    "  \"\"\"\n",
    "  Pre-processes the data one sample at a time using loops.\n",
    "  X has shape: (num_samples, training_vec_len)\n",
    "  Y has shape: (num_samples)\n",
    "  \"\"\"\n",
    "  processed_samples = []\n",
    "  y = []\n",
    "  skipped_no_picks = 0\n",
    "  skipped_downsample = 0\n",
    "\n",
    "  if log:\n",
    "    print(f'Processing {len(data)} samples')\n",
    "  \n",
    "  for i, sample in enumerate(data):\n",
    "\n",
    "    if log and i % log_freq == 0:\n",
    "      print(f'{((i / len(data)) * 100):.1f} % complete. => {i} of {len(data)}, '\n",
    "            f'skip_no_pick {skipped_no_picks}, '\n",
    "            f'skipped_downsample {skipped_downsample}')\n",
    "\n",
    "    if 'not_picked' not in sample or len(sample['not_picked']) == 0:\n",
    "        skipped_no_picks += 1\n",
    "        continue\n",
    "        \n",
    "    if sample['picked'] == 'Singing Bowl':\n",
    "        continue\n",
    "        \n",
    "    #if sample['picked'] == 'SKIP':\n",
    "     #   skipped_downsample += 1\n",
    "    #    continue\n",
    "        \n",
    "    if 'PrismaticShard' in sample['relics']:\n",
    "        continue\n",
    "        \n",
    "    if sample['max_hp'] > 200:\n",
    "        continue\n",
    "    \n",
    "    if USE_EMBEDDING:\n",
    "      processed_samples.append(encode_sample_embedding_with_loop(sample))\n",
    "    else:\n",
    "      processed_samples.append(encode_sample_with_loop(sample))\n",
    "    \n",
    "    #y.append(sample['damage_taken'])\n",
    "    y.append(encode_card_choice(sample))\n",
    "  X = np.vstack(processed_samples)\n",
    "  Y = np.array(y, dtype='float32')\n",
    "  return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xC-yUE1dFtzj"
   },
   "source": [
    "# Deep Learning\n",
    "\n",
    "The fun of the project! 😃 Let's build and training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v_lJoIQFEDxy"
   },
   "outputs": [],
   "source": [
    "# !pip install -U keras-tuner\n",
    "# import kerastuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F3a5NzjAhItw"
   },
   "source": [
    "## Scaling and Spliting Data\n",
    "\n",
    "Spliting the data into training and test set and scaling the features and labels to be between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJVsdCdWax50",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 141925 samples\n",
      "0.0 % complete. => 0 of 141925, skip_no_pick 0, skipped_downsample 0\n",
      "10.6 % complete. => 15000 of 141925, skip_no_pick 1832, skipped_downsample 0\n",
      "21.1 % complete. => 30000 of 141925, skip_no_pick 3658, skipped_downsample 0\n",
      "31.7 % complete. => 45000 of 141925, skip_no_pick 5520, skipped_downsample 0\n",
      "42.3 % complete. => 60000 of 141925, skip_no_pick 7371, skipped_downsample 0\n",
      "52.8 % complete. => 75000 of 141925, skip_no_pick 9226, skipped_downsample 0\n",
      "63.4 % complete. => 90000 of 141925, skip_no_pick 11061, skipped_downsample 0\n",
      "74.0 % complete. => 105000 of 141925, skip_no_pick 12911, skipped_downsample 0\n",
      "84.6 % complete. => 120000 of 141925, skip_no_pick 14766, skipped_downsample 0\n",
      "95.1 % complete. => 135000 of 141925, skip_no_pick 16616, skipped_downsample 0\n"
     ]
    }
   ],
   "source": [
    "if cached_data is False:\n",
    "  X, Y = preprocess_with_loop(json_data, log_freq=15000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_XSPY82uG1k"
   },
   "outputs": [],
   "source": [
    "def scale_X(X_data):\n",
    "  \"\"\"\n",
    "  Used with one hot encoded model\n",
    "  \"\"\"\n",
    "  X_copy = np.copy(X_data)\n",
    "  max_abs_scaler = MaxAbsScaler()\n",
    "  X_maxabs = max_abs_scaler.fit_transform(X_copy)\n",
    "  with open('input_scales.json', 'w') as out_file:\n",
    "    json.dump(max_abs_scaler.scale_.tolist(), out_file)\n",
    "  return X_maxabs\n",
    "\n",
    "def scale_Y(Y_data):\n",
    "  return Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'SKIP': 26952, 'Ball Lightning': 5151, 'Cold Snap': 4370, 'Coolheaded': 4357, 'Conserve Battery': 3399, 'Defragment': 2684, 'Glacier': 2531, 'Gash': 2468, 'Singing Bowl': 2144, 'Go for the Eyes': 2115, 'Self Repair': 2046, 'Hologram': 1975, 'Sweeping Beam': 1881, 'Capacitor': 1873, 'Echo Form': 1844, 'Leap': 1829, 'Electrodynamics': 1783, 'Chill': 1751, 'Loop': 1739, 'Streamline': 1701, 'Beam Cell': 1602, 'White Noise': 1484, 'Core Surge': 1473, 'Compile Driver': 1454, 'Biased Cognition': 1350, 'FTL': 1278, 'Barrage': 1248, 'Seek': 1240, 'Genetic Algorithm': 1210, 'Buffer': 1176, 'Creative AI': 1147, 'BootSequence': 1133, 'Doom and Gloom': 1120, 'Turbo': 1105, 'Sunder': 1102, 'Static Discharge': 1070, 'Reinforced Body': 1044, 'Steam': 1028, 'Melter': 998, 'Undo': 983, 'Consume': 944, 'Fission': 940, 'Redo': 925, 'Rebound': 890, 'Rainbow': 843, 'Darkness': 817, 'Chaos': 786, 'Lockon': 756, 'Auto Shields': 697, 'Machine Learning': 692, 'Reboot': 688, 'Storm': 685, 'All For One': 610, 'Recycle': 609, 'Tempest': 588, 'Skim': 559, 'Rip and Tear': 549, 'Thunder Strike': 506, 'Double Energy': 475, 'Blizzard': 473, 'Coolheaded+1': 469, 'Stack': 462, 'Amplify': 412, 'Heatsinks': 396, 'Conserve Battery+1': 359, 'Hologram+1': 358, 'Multi-Cast': 357, 'Fusion': 338, 'Force Field': 337, 'Ball Lightning+1': 333, 'Cold Snap+1': 305, 'Hyperbeam': 304, 'Go for the Eyes+1': 296, 'Leap+1': 296, 'Meteor Strike': 289, 'Redo+1': 281, 'Scrape': 264, 'Hello World': 252, 'Defragment+1': 221, 'Gash+1': 208, 'Barrage+1': 202, 'Glacier+1': 200, 'Loop+1': 192, 'Self Repair+1': 175, 'Compile Driver+1': 174, 'Sweeping Beam+1': 173, 'Steam+1': 171, 'Turbo+1': 167, 'Beam Cell+1': 167, 'Steam Power': 161, 'Capacitor+1': 160, 'White Noise+1': 150, 'Stack+1': 138, 'Streamline+1': 137, 'Chill+1': 125, 'Reinforced Body+1': 123, 'Storm+1': 111, 'Static Discharge+1': 109, 'Undo+1': 107, 'Sunder+1': 104, 'BootSequence+1': 103, 'Darkness+1': 101, 'Echo Form+1': 100, 'Rebound+1': 100, 'FTL+1': 99, 'Consume+1': 97, 'Tempest+1': 96, 'Genetic Algorithm+1': 94, 'Double Energy+1': 91, 'Skim+1': 85, 'Heatsinks+1': 84, 'Doom and Gloom+1': 83, 'Reprogram': 83, 'Creative AI+1': 80, 'Recycle+1': 80, 'Chaos+1': 79, 'Electrodynamics+1': 77, 'Force Field+1': 77, 'Buffer+1': 76, 'Lockon+1': 73, 'Blizzard+1': 72, 'Fusion+1': 71, 'Melter+1': 67, 'Auto Shields+1': 64, 'Biased Cognition+1': 63, 'Aggregate': 62, 'Core Surge+1': 60, 'Fission+1': 49, 'Rip and Tear+1': 40, 'Seek+1': 38, 'Steam Power+1': 37, 'Rainbow+1': 37, 'Machine Learning+1': 37, 'Multi-Cast+1': 37, 'Amplify+1': 36, 'All For One+1': 33, 'Scrape+1': 33, 'Reboot+1': 33, 'Thunder Strike+1': 32, 'Hello World+1': 20, 'Hyperbeam+1': 17, 'Meteor Strike+1': 15, 'Aggregate+1': 14, 'Reprogram+1': 6, 'Armaments': 5, 'Impervious': 4, 'Flex': 4, 'Anger': 3, 'Prostrate': 3, 'Well Laid Plans': 3, 'Iron Wave': 3, 'WheelKick': 3, 'Dash': 3, 'True Grit': 3, 'Deadly Poison': 3, 'Leg Sweep': 3, 'Devotion': 2, 'Envenom': 2, 'Panacea': 2, 'Perseverance': 2, 'Footwork': 2, 'LessonLearned': 2, 'Night Terror': 2, 'Master of Strategy': 2, 'Cleave': 2, 'CutThroughFate': 2, 'Flash of Steel': 2, 'Pommel Strike': 2, 'Corruption': 2, 'Outmaneuver': 2, 'Secret Weapon': 2, 'Thunderclap': 2, 'DevaForm': 2, 'Omniscience': 2, 'Searing Blow': 1, 'Consecrate+1': 1, 'Shrug It Off+1': 1, 'Blind': 1, 'InnerPeace': 1, 'Pray': 1, 'Consecrate': 1, 'Forethought': 1, 'Ghostly Armor': 1, 'Calculated Gamble': 1, 'Dramatic Entrance': 1, 'PiercingWail': 1, 'SashWhip+1': 1, 'Juggernaut': 1, 'Distraction': 1, 'Escape Plan+1': 1, 'Spot Weakness': 1, 'Vault': 1, 'BattleHymn+1': 1, 'Carnage': 1, 'Halt+1': 1, 'Backflip+1': 1, 'Bouncing Flask': 1, 'Sucker Punch': 1, 'Violence': 1, 'Offering': 1, 'Whirlwind': 1, 'Slice': 1, 'Prepared+1': 1, 'Dagger Spray': 1, 'PathToVictory': 1, 'Acrobatics+1': 1, 'Disarm': 1, 'Burst': 1, 'Shrug It Off': 1, 'WaveOfTheHand': 1, 'WindmillStrike': 1, 'Good Instincts+1': 1, 'Metamorphosis+1': 1, 'Double Tap': 1, 'Bandage Up': 1, 'SashWhip': 1, 'Evaluate': 1, 'FollowUp': 1, 'Seeing Red': 1, 'BattleHymn': 1, 'Intimidate': 1, 'Poisoned Stab': 1, 'Crippling Poison': 1, 'Feed': 1, 'Escape Plan': 1, 'Infernal Blade': 1, 'Blade Dance': 1, 'Metamorphosis': 1, 'Sanctity': 1, 'Adrenaline': 1, 'Infinite Blades': 1, 'Backflip': 1, 'Heel Hook': 1, 'Die Die Die': 1, 'Footwork+1': 1, 'Tools of the Trade': 1, 'Burning Pact': 1, 'Rage': 1, 'Conclude': 1, 'Bandage Up+1': 1, 'Entrench': 1, 'Flying Knee': 1, 'Rage+1': 1, 'Intimidate+1': 1, 'Twin Strike': 1, 'Cloak And Dagger+1': 1, 'Expertise': 1, 'Grand Finale': 1, 'Uppercut+1': 1, 'Riddle With Holes': 1, 'Reaper': 1, 'Feel No Pain': 1, 'CrushJoints': 1, 'Bullet Time': 1, 'TalkToTheHand': 1, 'Berserk': 1, 'SpiritShield': 1, 'Body Slam+1': 1, 'Dodge and Roll': 1, 'Calculated Gamble+1': 1, 'Cloak And Dagger': 1, 'BowlingBash+1': 1, 'JustLucky': 1, 'Wallop': 1, 'Iron Wave+1': 1, 'Protect': 1, 'FlyingSleeves': 1, 'Purity': 1, 'Havoc': 1, 'Blur': 1, 'Second Wind+1': 1, 'Body Slam': 1, 'Dark Shackles+1': 1, 'Shockwave+1': 1, 'Ragnarok': 1, 'Study': 1, 'Impervious+1': 1})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pick_histogram = collections.Counter()\n",
    "\n",
    "for game_state in json_data:\n",
    "    if 'picked' in game_state:\n",
    "        pick_histogram[game_state['picked']] += 1\n",
    "print(pick_histogram)\n",
    "print()\n",
    "common_labels_with_counts = collections.Counter(np.argmax(Y, axis=1))\n",
    "\n",
    "#print([(ALL_CARD_CHOICES[l], l, c) for l, c in common_labels_with_counts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U3qpj7egwxeo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                27450     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 296)               15096     \n",
      "=================================================================\n",
      "Total params: 45,096\n",
      "Trainable params: 45,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 65383 samples, validate on 16346 samples\n",
      "Epoch 1/20\n",
      "65383/65383 [==============================] - 4s 66us/sample - loss: 1.1198 - accuracy: 0.0131 - val_loss: 1.0716 - val_accuracy: 0.0017\n",
      "Epoch 2/20\n",
      "65383/65383 [==============================] - 3s 53us/sample - loss: 1.0581 - accuracy: 0.0019 - val_loss: 1.0444 - val_accuracy: 0.0016\n",
      "Epoch 3/20\n",
      "65383/65383 [==============================] - 3s 49us/sample - loss: 1.0350 - accuracy: 0.0021 - val_loss: 1.0381 - val_accuracy: 0.0017\n",
      "Epoch 4/20\n",
      "65383/65383 [==============================] - 3s 49us/sample - loss: 1.0203 - accuracy: 0.0017 - val_loss: 1.0319 - val_accuracy: 0.0016\n",
      "Epoch 5/20\n",
      "65383/65383 [==============================] - 3s 51us/sample - loss: 1.0093 - accuracy: 0.0018 - val_loss: 1.0402 - val_accuracy: 0.0013\n",
      "Epoch 6/20\n",
      "65383/65383 [==============================] - 3s 49us/sample - loss: 0.9990 - accuracy: 0.0015 - val_loss: 1.0306 - val_accuracy: 0.0015\n",
      "Epoch 7/20\n",
      "65383/65383 [==============================] - 3s 48us/sample - loss: 0.9908 - accuracy: 0.0016 - val_loss: 1.0302 - val_accuracy: 0.0015\n",
      "Epoch 8/20\n",
      "65383/65383 [==============================] - 3s 49us/sample - loss: 0.9825 - accuracy: 0.0020 - val_loss: 1.0323 - val_accuracy: 0.0021\n",
      "Epoch 9/20\n",
      "65383/65383 [==============================] - 3s 52us/sample - loss: 0.9789 - accuracy: 0.0019 - val_loss: 1.0369 - val_accuracy: 0.0015\n",
      "Epoch 10/20\n",
      "65383/65383 [==============================] - 3s 50us/sample - loss: 0.9691 - accuracy: 0.0018 - val_loss: 1.0344 - val_accuracy: 0.0013\n",
      "Epoch 11/20\n",
      "65383/65383 [==============================] - 3s 50us/sample - loss: 0.9679 - accuracy: 0.0015 - val_loss: 1.0323 - val_accuracy: 0.0012\n",
      "Epoch 12/20\n",
      "65383/65383 [==============================] - 4s 58us/sample - loss: 0.9597 - accuracy: 0.0018 - val_loss: 1.0393 - val_accuracy: 0.0015\n",
      "Epoch 13/20\n",
      "65383/65383 [==============================] - 4s 66us/sample - loss: 0.9563 - accuracy: 0.0013 - val_loss: 1.0403 - val_accuracy: 9.1766e-04\n",
      "Epoch 14/20\n",
      "65383/65383 [==============================] - 4s 61us/sample - loss: 0.9518 - accuracy: 0.0014 - val_loss: nan - val_accuracy: 9.7883e-04\n",
      "Epoch 15/20\n",
      "65383/65383 [==============================] - 3s 53us/sample - loss: 0.9493 - accuracy: 0.0016 - val_loss: nan - val_accuracy: 0.0013\n",
      "Epoch 16/20\n",
      "65383/65383 [==============================] - 3s 50us/sample - loss: 0.9453 - accuracy: 0.0014 - val_loss: nan - val_accuracy: 0.0012\n",
      "Epoch 17/20\n",
      "65383/65383 [==============================] - 4s 63us/sample - loss: 0.9435 - accuracy: 0.0015 - val_loss: nan - val_accuracy: 0.0010\n",
      "Epoch 18/20\n",
      "65383/65383 [==============================] - 3s 48us/sample - loss: 0.9392 - accuracy: 0.0014 - val_loss: nan - val_accuracy: 9.1766e-04\n",
      "Epoch 19/20\n",
      "65376/65383 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0113"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Nan in summary histogram for: dense/kernel_0 [Op:WriteHistogramSummary] name: dense/kernel_0/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ccafaa602042>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m history = model.fit(X_train, Y_train, \n\u001b[1;32m     32\u001b[0m                     \u001b[0;31m#sample_weight=weights_train,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                     batch_size=32, epochs=20, validation_split=0.2, callbacks=[tensorboard_callback])\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;31m# Epochs only apply to `fit`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m     \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram_freq\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1613\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_freq\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_log_weights\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m   1691\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m             \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1693\u001b[0;31m           \u001b[0msummary_ops_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1694\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_weight_as_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36mhistogram\u001b[0;34m(name, tensor, family, step)\u001b[0m\n\u001b[1;32m    799\u001b[0m         name=scope)\n\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msummary_writer_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfamily\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36msummary_writer_function\u001b[0;34m(name, tensor, function, family)\u001b[0m\n\u001b[1;32m    728\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     op = smart_cond.smart_cond(\n\u001b[0;32m--> 730\u001b[0;31m         should_record_summaries(), record, _nothing, name=\"\")\n\u001b[0m\u001b[1;32m    731\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m       \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/smart_cond.py\u001b[0m in \u001b[0;36msmart_cond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpred_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpred_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36mrecord\u001b[0;34m()\u001b[0m\n\u001b[1;32m    721\u001b[0m     with ops.name_scope(name_scope), summary_op_util.summary_scope(\n\u001b[1;32m    722\u001b[0m         name, family, values=[tensor]) as (tag, scope):\n\u001b[0;32m--> 723\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(tag, scope)\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m         name=scope)\n\u001b[0m\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msummary_writer_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfamily\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_summary_ops.py\u001b[0m in \u001b[0;36mwrite_histogram_summary\u001b[0;34m(writer, step, tag, values, name)\u001b[0m\n\u001b[1;32m    584\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         return write_histogram_summary_eager_fallback(\n\u001b[0;32m--> 586\u001b[0;31m             writer, step, tag, values, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m    587\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_summary_ops.py\u001b[0m in \u001b[0;36mwrite_histogram_summary_eager_fallback\u001b[0;34m(writer, step, tag, values, name, ctx)\u001b[0m\n\u001b[1;32m    620\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m   _result = _execute.execute(b\"WriteHistogramSummary\", 0, inputs=_inputs_flat,\n\u001b[0;32m--> 622\u001b[0;31m                              attrs=_attrs, ctx=_ctx, name=name)\n\u001b[0m\u001b[1;32m    623\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Nan in summary histogram for: dense/kernel_0 [Op:WriteHistogramSummary] name: dense/kernel_0/"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(50, input_shape=(X.shape[1],), activation='relu'),\n",
    "  tf.keras.layers.Dropout(.2),\n",
    "  tf.keras.layers.Dense(50, activation='relu'),\n",
    "  tf.keras.layers.Dropout(.2),\n",
    "  tf.keras.layers.Dense(len(ALL_CARD_CHOICES), activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "#model.compile(\n",
    "#    optimizer=keras.optimizers.RMSprop(learning_rate=.001),\n",
    "#    loss='mean_absolute_error',\n",
    "#    metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "model.compile(\n",
    "    loss=masked_cce,\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    "   )\n",
    "\n",
    "# Tensorboard\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "#X_scaled = scale_X(X)\n",
    "X_scaled = X # try this?\n",
    "Y_scaled = scale_Y(Y)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_scaled, Y_scaled, test_size=0.33, shuffle=False)\n",
    "\n",
    "history = model.fit(X_train, Y_train, \n",
    "                    #sample_weight=weights_train,\n",
    "                    batch_size=32, epochs=20, validation_split=0.2, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=32, epochs=5, validation_split=0.2, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0bK-_jVZGEQ"
   },
   "outputs": [],
   "source": [
    "test_scores = model.evaluate(X_test, Y_test, verbose=2)\n",
    "\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_options_out = np.multiply(out, np.absolute(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cn(index):\n",
    "    return ALL_CARD_CHOICES[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = 0\n",
    "\n",
    "for example, predictions, actuals in zip(X_test, out, Y_test):\n",
    "    model_pick = np.argmax(np.multiply(predictions, np.absolute(actuals)))\n",
    "    real_label = np.argmax(actuals)\n",
    "    if model_pick == real_label: \n",
    "        print('correct')\n",
    "    else: \n",
    "        print('wrong')\n",
    "    choice_indexes =  np.nonzero(actuals)[0]\n",
    "    choices = list(map(cn, choice_indexes))\n",
    "    print(choices, cn(model_pick), cn(real_label))\n",
    "    print(decode_encoded_sample(example))\n",
    "    ct += 1\n",
    "    print()\n",
    "    if ct > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = collections.Counter(np.argmax(np.multiply(out, np.absolute(Y_test)), axis=1) )\n",
    "c.most_common(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w0fvZvGAjJ1O"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zen-W27CYH_B"
   },
   "source": [
    "### Tensorboard\n",
    "\n",
    "Inspect training and validation results with tenorboard and upload logs to tensorboard.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zk60wxTWQM0q"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Bh7qBkRnWzr"
   },
   "outputs": [],
   "source": [
    "# !tensorboard dev upload --logdir ./logs \\\n",
    "#   --name \"Slay the Spire fight predictions\" \\\n",
    "#   --description \"Training results predicting health loss in a Slay the Spire fight\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "USd4CSibYjQ7"
   },
   "source": [
    "### Manual Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1Nga2iI2nnf"
   },
   "outputs": [],
   "source": [
    "def inspect_test_cases():\n",
    "  \"\"\"\n",
    "  For manually looking at test predictions vs actual number\n",
    "  \"\"\"\n",
    "  num_test = 50\n",
    "  print(X_test.shape)\n",
    "  pred = model.predict(X_test[:num_test])\n",
    "  pred_actual = Y_test[:num_test]\n",
    "  pred_actual = pred_actual[..., np.newaxis]\n",
    "  out = np.concatenate((pred, pred_actual), axis=1)\n",
    "  np.set_printoptions(precision=3, suppress=True)\n",
    "  print(out)\n",
    "inspect_test_cases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ysor4liQUGBM"
   },
   "source": [
    "## Tune the Model\n",
    "\n",
    "Let's try to find the best hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mP_XrW4Hvp0-"
   },
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "  model = tf.keras.models.Sequential()\n",
    "  for i in range(hp.Int('num_layers_big', 1, 2)):\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        units=hp.Int('units_' + str(i), min_value=100, max_value=500, step=50, default=400), activation='relu'))\n",
    "    tf.keras.layers.Dropout(\n",
    "      hp.Float('dropout', 0.1, 0.4, step=0.1, default=0.2))\n",
    "  for i in range(hp.Int('num_layers_small', 1, 2)):\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        units=hp.Int('units_' + str(i), min_value=16, max_value=128, step=16, default=32), activation='relu'))\n",
    "    tf.keras.layers.Dropout(\n",
    "      hp.Float('dropout', 0, 0.3, step=0.1, default=0.2))\n",
    "  model.add(tf.keras.layers.Dense(1))\n",
    "  model.compile(\n",
    "      optimizer=keras.optimizers.RMSprop(hp.Choice('learning_rate', [1e-1, 1e-2, 1e-3, 1e-4])),\n",
    "      loss='mean_absolute_error',\n",
    "      metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "  return model\n",
    "\n",
    "def tune_model():\n",
    "  tuner = kt.Hyperband(\n",
    "      build_model,\n",
    "      objective='val_loss',\n",
    "      max_epochs=20,\n",
    "      hyperband_iterations=4)\n",
    "  tuner.search(X_train, Y_train,\n",
    "              epochs=5,\n",
    "              validation_data=(X_test, Y_test))\n",
    "  return tuner\n",
    "\n",
    "# tuner = tune_model()\n",
    "# tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jm3SjKvhkmhp"
   },
   "source": [
    "## Save the Model\n",
    "\n",
    "Save the model to be loaded into the game to be used in a mod!\n",
    "\n",
    "Cache the data to help speed up development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5cdL-3DClRN4"
   },
   "outputs": [],
   "source": [
    "model.save(\"STSFightPredictor\") # Saved Model Format\n",
    "# model.save(\"STSFP.h5\")\n",
    "\n",
    "np.savez_compressed('cached_comp_data.npz', X=X, Y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sy3XtuQtVxQG"
   },
   "outputs": [],
   "source": [
    "# For testing in the mod that the model in the mod is predicting the save values\n",
    "for i in range(3):\n",
    "  case = X_test[i]\n",
    "  sb = ''\n",
    "  sb = 'float[] testCase = {'\n",
    "  for num in case:\n",
    "    sb += str(num)\n",
    "    sb += '0f, '\n",
    "  sb += '};'\n",
    "  print(sb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2j1R0um7Uwlr"
   },
   "source": [
    "## Embedding Experiments\n",
    "\n",
    "The current model has a good training loss curve but the validation curve looks more like a straight line than a curve. This problem is often related to overfit. Rather than one hot encoding cards, relics, and encounter, an experiment with Embedding layers was run to try to reduce overfit.\n",
    "\n",
    "An Embedding layer can be used to learn relations between cards. The general idea is to encode cards, relics, and enemies as vectors instead of a single numbers.\n",
    "\n",
    "Because there are a variable number of cards and relics a player can have, the average of card vectors and the average of relic vectors was taken. These averages can be passed into a Dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bVxFXMT9V9BO"
   },
   "outputs": [],
   "source": [
    "# Custom average function to ignore masked layers\n",
    "def avg_labmda_fun(x, mask):\n",
    "  mask_cast = keras.backend.cast(mask, 'float32')\n",
    "  expanded = keras.backend.expand_dims(mask_cast)\n",
    "  count = tf.keras.backend.sum(mask_cast)\n",
    "  sum = keras.backend.sum(expanded * x, axis=1)\n",
    "  return sum / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gdltqaZu5yC_"
   },
   "outputs": [],
   "source": [
    "def train_embedding_model():\n",
    "  # Embed cards and average output vectors\n",
    "  card_input = Input(shape=(NUM_CARDS_FOR_EMB, ), name='cards_input')\n",
    "  card_embedding = Embedding(len(ALL_CARDS) + 1, 26, mask_zero=True)(card_input)\n",
    "  card_average = Lambda(avg_labmda_fun, output_shape=(26, ), mask=None)(card_embedding)\n",
    "\n",
    "  # Embed relics and average output vectors\n",
    "  relic_input = Input(shape=(NUM_RELICS_FOR_EMB, ), name='relics_input')\n",
    "  relic_embedding = Embedding(len(ALL_RELICS) + 1, 13, mask_zero=True)(relic_input)\n",
    "  relic_average = Lambda(avg_labmda_fun, output_shape=(13, ), mask=None)(relic_embedding)\n",
    "\n",
    "  # Embed encounter. There is only a single encounter but the lambda is used to reshape the vector\n",
    "  encounter_input = Input(shape=(1, ), name='encounter_input')\n",
    "  encounter_embedding = Embedding(len(ALL_ENCOUNTERS), 8)(encounter_input)\n",
    "  encounter_layer_reshape = Lambda(lambda x: keras.backend.mean(x, axis=1), output_shape=(8, ))(encounter_embedding)\n",
    "\n",
    "  numbers_input = Input(shape=(4, ), name='num_and_bool_input')\n",
    "\n",
    "  # Concatenate before sending to Dense layers\n",
    "  merged = concatenate([card_average, relic_average, encounter_layer_reshape, numbers_input])\n",
    "\n",
    "  dense_1 = Dense(40, activation='relu')(merged)\n",
    "  drop_out_1 = Dropout(.1)(dense_1)\n",
    "  dense_out = Dense(1)(drop_out_1)\n",
    "\n",
    "\n",
    "  emb_model = Model(inputs=[card_input, relic_input, encounter_input, numbers_input], output=dense_out)\n",
    "  emb_model.summary()\n",
    "\n",
    "  emb_model.compile(\n",
    "      optimizer=keras.optimizers.RMSprop(learning_rate=.0001),\n",
    "      loss='mse',\n",
    "      metrics=['mae'])\n",
    "\n",
    "  logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "  Y_scaled = scale_Y(Y)\n",
    "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y_scaled, test_size=0.33, shuffle=False)\n",
    "\n",
    "  cards_col = X_train[:, 0:NUM_CARDS_FOR_EMB]\n",
    "  relic_index = NUM_CARDS_FOR_EMB + NUM_RELICS_FOR_EMB\n",
    "  relics_col = X_train[:, NUM_CARDS_FOR_EMB:relic_index]\n",
    "  encounter_index = relic_index + 1\n",
    "  encounter_col = X_train[:, relic_index:encounter_index]\n",
    "  num_and_bool_col = X_train[:, encounter_index:]\n",
    "  max_abs_scaler = MaxAbsScaler()\n",
    "  num_and_bool_col = max_abs_scaler.fit_transform(num_and_bool_col)\n",
    "\n",
    "  history = emb_model.fit(x={'cards_input': cards_col, 'relics_input': relics_col, 'encounter_input': encounter_col, 'num_and_bool_input': num_and_bool_col}, y=Y_train, batch_size=32, epochs=20, validation_split=0.2)\n",
    "  return emb_model\n",
    "\n",
    "if USE_EMBEDDING:\n",
    "  emb_model = train_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i87upB6aacyd"
   },
   "outputs": [],
   "source": [
    "def inspect_embeddings():\n",
    "  embeddings = emb_model.layers[3].get_weights()[0]\n",
    "  embeddings.shape\n",
    "  weights = dict()\n",
    "  for i, name in enumerate(ALL_CARDS):\n",
    "    weights[name] = embeddings[i]\n",
    "    # print(f'{name}:\\t{embeddings[i]}')\n",
    "\n",
    "  print(weights['Pommel Strike'])\n",
    "  print(weights['Sucker Punch'])\n",
    "\n",
    "if USE_EMBEDDING:\n",
    "  inspect_embeddings()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMUnm7mY8pL9wA+4TEzmbft",
   "include_colab_link": true,
   "name": "STSFightPredictor.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
